---
title: "MAST30027_Assignment2"
author: "Zi Ng (1085130)"
date: "Thursday 4.15 pm, Anubhav Kaphle"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(faraway)
```

# Question 1
Firstly we will read and inspect the data. There are 70 observations. We aim to determine which factors (duration, residence, education) and two-way interactions are related to the number of children per woman (fertility rate). 
```{r}
data <- read.table(file ="assignment2_prob1.txt", header=TRUE)
data$duration <- factor(data$duration, 
                        levels=c("0-4","5-9","10-14","15-19","20-24","25-29"),
                        ordered=TRUE)
data$residence <- factor(data$residence, levels=c("Suva", "urban", "rural"))
data$education <- factor(data$education, levels=c("none", "lower", "upper", "sec+"))
data$fertility <- data$nChildren / data$nMother
str(data)
# ftable(xtabs(cbind(nChildren,nMother,fertility) ~ 
#                duration + residence + education, data))
```

We can visualize the data with pair plots. Visually, we can roughly make out a relationships between fertility and duration, residence as well as education. 
```{r}
with(data, pairs(fertility ~ duration + residence + education))
```

We can also use interaction plots to see if there are two-way relationships related to the fertility rate. Since these slopes are not quite parallel, it appears that the two-way interactions might impact fertility rate. 
```{r}
par(mfrow=c(1,3))
with(data, interaction.plot(residence, duration, fertility))
with(data, interaction.plot(education, duration, fertility))
with(data, interaction.plot(residence, education, fertility))
```

Since the number of children a woman has is count data, it makes sense to fit a Poisson model. Since the number of children depends on the number of women, we can model the rate per unit in the form of a Poisson glm with log link. 
\begin{align*}
\log(\lambda_i / t_i) &= x_i^T \beta \\
\log(\lambda_i) &= \log(t_i) + x_i^T \beta
\end{align*}
We can fit the rate model using the glm command with offset to constrain the coefficient of $\log(t_i)$ to 1.
```{r}
model = glm(nChildren ~ offset(log(nMother)) + duration + residence + education + 
              duration*residence + duration*education + education*residence, 
            family = poisson, data = data)
summary(model)
```
For model selection, we will use the step function based on the AIC. 
```{r}
model.step = step(model, scope = ~.)
summary(model.step)
```

Next, we will perform diagnostics. 
```{r}
par(mfrow=c(2,2))
# data points look cramped 
plot(residuals(model.step) ~ predict(model.step,type="response"))
# data points look OK 
plot(residuals(model.step) ~ predict(model.step, type="link"))
# data points look OK
plot(residuals(model.step, type="pearson") ~ predict(model.step, type="link"))
# appear to be heteroskedastic 
plot(residuals(model.step, type="response") ~ predict(model.step, type="link"))
```

We will examine the data points to find outliers and points with significant impact. Based on the graphs, it looks like observations 17 and 57 are potential outliers. Observation 17 looks like it might still belong on the smooth curve, however observation 57 clearly deviates from the curve. We will refit a model that excludes observation 57 to see if it changes the model.
```{r}
par(mfrow=c(2,2))
plot(predict(model.step), residuals(model.step))
halfnorm(residuals(model.step), ylab="residuals")
halfnorm(rstudent(model.step), ylab="jacknife resid")
halfnorm(cooks.distance(model.step), ylab="cooks dist")
```

```{r}
model.subset = glm(nChildren ~ offset(log(nMother)) + duration + residence + education + 
                     duration*residence + duration*education + education*residence, 
                   family = poisson, data = data, subset = c(-57))
model.subset.step = step(model.subset, scope=~., trace=0)
summary(model.subset.step)
```

Apparently, removing observation 57 does not have a large impact on the model. Hence we will revert to the original model without subsetting the data. 

Checking the scaled deviance, we see verify that the model is appropriate. 
```{r}
anova(model.step)
pchisq(deviance(model.step), 59, lower.tail = FALSE)
```

Finally, we can check for overdispersion by estimating phi. Since $\hat{\phi} \approx 1$, there is no overdispersion. 
```{r}
(phihat <- sum(residuals(model.step, type="pearson")^2) / 59)
```

Hence, we can model the number children born to married women of the Indian race using a Poisson model with variables including the marriage duration of mothers, the residence of families in each group and the education of the mothers in each group. 

\newpage

# Question 2
#### 2a 
Expectation of the complete log-likelihood. 

The likelihood of observing $X = (X_1, ..., X_{300})$ and $Z = (Z_1, ..., Z_{300})$ given $\theta=(\pi_1, \pi_2, p_1, p_2, p_3)$ is calculated, noting that $\pi_3 = 1-\pi_1-\pi_2$.
\begin{align*}
& \Pr(X_1, ..., X_{300}, Z_1, ..., Z_{300} | \theta) \\
=& \Pi_{i=1}^{300} {\Pr(X_i | Z_i, \theta) \cdot \Pr(Z_i | \theta)} \\
=& \Pi_{i=1}^{300} \Pi_{j=1}^{3} [\Pr(X_i | Z_i=j, \theta) \cdot \Pr(Z_i=j | \theta)]^{I(Z_i=j)}
\end{align*}

The log-likelihood is then calculated.
\begin{align*}
& \log \Pr(X_1, ..., X_{300}, Z_1, ..., Z_{300} | \theta) \\
=& \sum_{i=1}^{300} {\sum_{j=1}^3 {I(Z_i=j) (\log(\Pr(X_i | Z_i=j, \theta)) + \log(\Pr(Z_i=j | \theta)))}} \\
=& \sum_{i=1}^{300} {\sum_{j=1}^3 {I(Z_i=j) (\log{20 \choose x_i} \cdot p_j^{x_i} \cdot (1-p_j)^{20-x_i} + \log{\pi_j})}} \\
=& \sum_{i=1}^{300} {\sum_{j=1}^3 {I(Z_i=j) (x_i \cdot \log{p_j} + (20-x_i)\log(1-p_j) + \log{20 \choose x_i} + \log{\pi_j}}}) \\
\end{align*}

Finally, we can take the expectation to derive the complete log-likelihood. 
\begin{align*}
& Q(\theta, \theta^0) \\
=& E_{Z | X, \theta^0} [\log(\Pr(X, Z | \theta))] \\
=& \sum_{i=1}^{300} [\sum_{j=1}^3 {\Pr(Z_i=j | X, \theta^0) (x_i \log{p_j} + (20-x_i)\log(1-p_j) + \log{20 \choose x_i} + \log{\pi_j})}] 
\end{align*}

\newpage

#### 2b 
The E-step of the EM algorithm. 

Using $\theta^0 = (\pi_1^0, \pi_2^0, p_1^0, p_2^0, p_3^0)$, we can derive the posterior distribution of the latent variables, where $\pi_3 = 1-\pi_1-\pi_2$.
\begin{align*}
& \Pr(Z_i=j | X, \theta^0) \\
=& \frac{\Pr(Z_i = j, X_i | \theta^0)}{\Pr(X_i | \theta^0)} \\
=& \frac{\Pr(X_i | Z_i=j, \theta^0) \Pr(Z_i=j | \theta^0)} {\sum_{k=1}^3 {\Pr(X_i | Z_i=k, \theta^0) \Pr(Z_i=k | \theta^0)}} \\
=& \frac{{20 \choose x_i} p_j^{x_i} (1-p_j)^{20-x_i} \pi_j}{\sum_{k=1}^3 {{20 \choose x_i} p_k^{x_i} (1-p_k)^{20-x_i} \pi_k}}
\end{align*}

\newpage

#### 2c 
The M-step of the EM algorithm. 

Firstly, derive the new estimate of $\pi_j$, for $j=1,2$. Based on the working shown during the lectures, the estimate of $pi_j$ can be derived. 
\begin{align*}
\frac{\partial Q(\theta, \theta^0)}{\partial \pi_j} &= 0 \\
\sum_{i=1}^{300} [\frac{\Pr(Z_i=j | X,\theta^0)}{\pi_j} - \frac{\Pr(Z_i=k | Z,\theta^0)}{1-\pi_1-\pi_2}] &= 0 
\end{align*}
\begin{align*}
\hat{\pi_j} &= \frac{1}{300} P(Z_i=j | X, \theta^0)
\end{align*}

Secondly, derive the new estimate of $p_j$, for $j=1,2,3$. 
\begin{align*}
\frac{\partial Q(\theta, \theta^0)}{\partial p_j} &= 0 \\
\sum_{i=1}^{300} {\Pr(Z_i=j | X, \theta^0) (\frac{x_i}{p_j} - \frac{20-x_i}{1-p_j})} &= 0 \\
\sum_{i=1}^{300} {\Pr(Z_i=j | X, \theta^0) (x_i(1-p_j) - (20-x_i)p_j)} &= 0 \\
\sum_{i=1}^{300} {\Pr(Z_i=j | Z,\theta^0) x_i} - 20\cdot p_j \sum_{i=1}^{300} {\Pr(Z_i=j | Z,\theta^0)} &= 0 \\
\hat{p_j} = \frac{\sum_{i=1}^{300} {\Pr(Z_i=j | X,\theta^0) x_i}}{20 \sum_{i=1}^{300} {\Pr(Z_i=j | X,\theta)}} 
\end{align*}

\newpage

#### 2d
Implement the EM algorithm and obtain MLE of the paramenters.

Read the data
```{r}
X = scan(file="assignment2_prob2.txt", what=double())
length(X)
hist(X)
```

Implementation of the EM algorithm
```{r}
mixture.EM = function(X, w.init, p.init, epsilon=1e-5, max.iter=100) {
  
  # initialize current parameter values
  w.curr = w.init
  p.curr = p.init
  
  # compute incomplete log=likelihoods using intial value of parameters. 
  log_liks = c()
  log_liks = c(log_liks, compute.log.lik(X, w.curr, p.curr)$ill)
  
  # change in incomplete log-likelihood
  delta.ll = 1
  
  # number of iterations
  n.iter = 1
  
  # If the log-likelihood has changed by less than epsilon, EM will stop
  while ((delta.ll > epsilon) & (n.iter <= max.iter)) {
    
    # run the EM step 
    EM.out = EM.iter(X, w.curr, p.curr)
    
    # replace the current parameter estimates
    w.curr = EM.out$w.new
    p.curr = EM.out$p.new
    
    # compute the change in incomplete log-likelihood 
    log_liks = c(log_liks, compute.log.lik(X, w.curr, p.curr)$ill)
    delta.ll = log_liks[length(log_liks)] - log_liks[length(log_liks) - 1]
    
    # increase the number of iterations
    n.iter = n.iter + 1
  }
  return(list(w.curr=w.curr, p.curr=p.curr, log_liks=log_liks))
}

# EM-iteration
EM.iter = function(X, w.curr, p.curr) {
  
  # E-step
  prob.x.z = compute.prob.x.z(X, w.curr, p.curr)$prob.x.z
  P_ik = prob.x.z / rowSums(prob.x.z)
  
  # M-step
  w.new = colSums(P_ik) / sum(P_ik)
  p.new = colSums(P_ik * X) / colSums(P_ik) / 20
  
  return(list(w.new=w.new, p.new=p.new))
}

# Incomplete log-likelihoods
compute.log.lik = function(X, w.curr, p.curr) {
  
  # compute probabilities
  prob.x.z = compute.prob.x.z(X, w.curr, p.curr)$prob.x.z
  
  # incomplete log-likelihoods
  ill = sum(log(rowSums(prob.x.z)))
  
  return(list(ill=ill))
}

# Compute probabilities
compute.prob.x.z = function(X, w.curr, p.curr) {
  
  L = matrix(NA, nrow=length(X), ncol=length(w.curr)) 
  for (k in seq_len(ncol(L))) {
    L[,k] = dbinom(X, size=20, prob=p.curr[k]) * w.curr[k]
  }
  return(list(prob.x.z=L))
}
```

\newpage

Apply the EM algorithm
```{r}
EM1 = mixture.EM(X, w.init=c(0.3,0.3,0.4), p.init=c(0.2, 0.5, 0.7))
EM2 = mixture.EM(X, w.init=c(0.1,0.2,0.7), p.init=c(0.1, 0.3, 0.7))
```

Print results 
```{r}
print.results <- function(EM) {
  print(paste("Estimate pi = (", round(EM$w.curr[1],2), ",", 
            round(EM$w.curr[2],2), ",",
            round(EM$w.curr[3],2), ")", sep=""))
  print(paste("Estimate p = (", round(EM$p.curr[1],2), ",", 
              round(EM$p.curr[2],2), ",",
              round(EM$p.curr[3],2), ")", sep=""))
  plot(EM$log_liks, ylab="incomplete log-likelihood", xlab="iteration")
}
print.results(EM1)
print.results(EM2)
```

\newpage

# Question 3
#### 3a 
The expectation of the complete log-likelihood. 

The likelihood of observing $X = (X_1, ..., X_{300}, X_{301}, ..., X_{400})$ and $Z = (Z_1, ..., Z_{300})$ given $\theta=(\pi_1, \pi_2, p_1, p_2, p_3)$ is calculated, noting that $\pi_3 = 1-\pi_1-\pi_2$.
\begin{align*}
& \Pr(X_1, ..., X_{300}, X_{301}, ..., X_{400}, Z_1, ..., Z_{300} | \theta) \\
=& \Pi_{i=1}^{300} [\Pr(X_i | Z_i, \theta) \cdot \Pr(Z_i | \theta)] \Pi_{i=301}^{400} \Pr(X_j | \theta) \\
=& \Pi_{i=1}^{300} \Pi_{j=1}^{3} [\Pr(X_i | Z_i=j, \theta) \cdot \Pr(Z_i=j | \theta)]^{I(Z_i=j)} \cdot \Pi_{i=301}^{400} \Pr(X_j | \theta)
\end{align*}

The log-likelihood is then calculated.
\begin{align*}
& \log \Pr(X_1, ..., X_{300}, Z_1, ..., Z_{300} | \theta) \\
=& \sum_{i=1}^{300} [\sum_{j=1}^3 {I(Z_i=j) (\log(\Pr(X_i | Z_i=j, \theta)) + \log(\Pr(Z_i=j | \theta)))}] + \sum_{k=301}^{400} \log(\Pr(X_k | \theta)) \\
=& \sum_{i=1}^{300} [\sum_{j=1}^3 {I(Z_i=j) (\log{20 \choose x_i} \cdot p_j^{x_i} \cdot (1-p_j)^{20-x_i} + \log{\pi_j})}] + \sum_{k=301}^{400} [\log({20 \choose x_k} p_1^{x_k} (1-p_1)^{20-x_k})] \\
=& \sum_{i=1}^{300} {\sum_{j=1}^3 [I(Z_i=j) (x_i \cdot \log{p_j} + (20-x_i)\log(1-p_j) + \log{20 \choose x_i} + \log{\pi_j})}] + \\ 
& \sum_{k=301}^{400} [log{20 \choose x_k} + x_k \log{p_1} + (20-x_k)\log(1-p_1)] \\
\end{align*}

Finally, we can take the expectation to derive the complete log-likelihood. 
\begin{align*}
& Q(\theta, \theta^0) \\
=& E_{Z | X, \theta^0} [\log(\Pr(X, Z | \theta))] \\
=& \sum_{i=1}^{300} [\sum_{j=1}^3 {\Pr(Z_i=j | X, \theta^0) (x_i \log{p_j} + (20-x_i)\log(1-p_j) + \log{20 \choose x_i} + \log{\pi_j})}] + \\
& \sum_{k=301}^{400} [\log{20 \choose x_k} + x_k \log{p_1} + (20-x_k) \log(1-p_1)]
\end{align*}

\newpage

#### 3b
Derive E-step and M-step of the EM algorithm. 

Firstly, we compute the E-step. 
\begin{align*}
& \Pr(Z_i=j | X, \theta^0) \\
=& \frac{\Pr(Z_i = j, X_i | \theta^0)}{\Pr(X_i | \theta^0)} \\
=& \frac{\Pr(X_i | Z_i=j, \theta^0) \Pr(Z_i=j | \theta^0)} {\sum_{k=1}^3 {\Pr(X_i | Z_i=k, \theta^0) \Pr(Z_i=k | \theta^0)}} \\
=& \frac{{20 \choose x_i} p_j^{x_i} (1-p_j)^{20-x_i} \pi_j}{\sum_{k=1}^3 {{20 \choose x_i} p_k^{x_i} (1-p_k)^{20-x_i} \pi_k}}
\end{align*}

Secondly, we compute the M-step. The proportion estimates are similar to the previous derivation. 
\begin{align*}
\frac{\partial Q(\theta, \theta^0)}{\partial \pi_j} &= 0 \\
\sum_{i=1}^{300} [\frac{\Pr(Z_i=1 | X,\theta^0)}{\pi_1} - \frac{\Pr(Z_i=3 | Z,\theta^0)}{1-\pi_1-\pi_2}] &= 0 
\end{align*}
\begin{align*}
\hat{\pi_j} &= \frac{1}{300} P(Z_i=j | X, \theta^0)
\end{align*}

We differentiate w.r.t. $p_1$ to obtain the new parameter estimates for $p_1$. 
\begin{align*}
\frac{\partial Q(\theta, \theta^0)}{\partial p_1} &= 0 \\
\sum_{i=1}^{300} [\Pr(Z_i=1 | X,\theta)(x_i(1-p_1) - (20-x_i)p_1)] + \sum_{i=301}^{400} [x_k(1-p_1) - (20-x_k)p_1] &= 0 \\
\sum_{i=1}^{300} {\Pr(Z_i=1 | X_i,\theta) x_i} - 20 \sum_{i=1}^{300} {\Pr(Z_i=1 | X_i,\theta) p_1} + \sum_{i=301}^{400} {x_k} - 20 \sum_{i=301}^{400} {p_1} &= 0 
\end{align*}
\begin{align*}
\hat{p_1} &= \frac {\sum_{i=1}^{300} {\Pr(Z_i=1 | X_i,\theta) x_i} + \sum_{k=301}^{400} {x_k}} {20 (\sum_{i=1}^{300} {\Pr(Z_i=1 | X_i,\theta) + 100)}}
\end{align*}

Finally, we compute the new parameter estimates for $p_2$ and $p_3$. 
\begin{align*}
\frac{\partial Q(\theta, \theta^0)}{\partial p_2} = 0 \\
\sum_{i=1}^{300} {\Pr(Z_i=2 | X, \theta^0) (\frac{x_i}{p_2} - \frac{20-x_i}{1-p_2})} = 0 \\
\sum_{i=1}^{300} {\Pr(Z_i=2 | X, \theta^0) (x_i(1-p_2) - (20-x_i)p_2)} = 0 \\
\sum_{i=1}^{300} {\Pr(Z_i=2 | Z,\theta^0) x_i} - 20\cdot p_2 \sum_{i=1}^{300} {\Pr(Z_i=2 | Z,\theta^0)} = 0 \\
\hat{p_2} = \frac{\sum_{i=1}^{300} {\Pr(Z_i=2 | X,\theta^0) x_i}}{20 \sum_{i=1}^{300} {\Pr(Z_i=2 | X,\theta^0)}} \\
\hat{p_3} = \frac{\sum_{i=1}^{300} {\Pr(Z_i=3 | X,\theta^0) x_i}}{20 \sum_{i=1}^{300} {\Pr(Z_i=3 | X,\theta^0)}} 
\end{align*}

\newpage

#### 3c 
Implement and run the EM algorithm. 

Read the data
```{r}
X = scan(file="assignment2_prob2.txt", what=double())
X.more = scan(file="assignment2_prob3.txt", what=double()) 
length(X)
length(X.more)
par(mfrow=c(2,2))
hist(X, xlim=c(0,20), ylim=c(0,80))
hist(X.more, xlim=c(0,20), ylim=c(0,80))
hist(c(X,X.more), xlim=c(0,20), ylim=c(0,80), xlab="X + X.more", 
     main = "Histogram of X + X.more")
```

Implementation 
```{r}
mixture.EM = function(X, X.more, w.init, p.init, epsilon=1e-5, max.iter=100) {
  
  # initialize current parameter values
  w.curr = w.init
  p.curr = p.init
  
  # compute incomplete log=likelihoods using intial value of parameters. 
  log_liks = c()
  log_liks = c(log_liks, compute.log.lik(X, X.more, w.curr, p.curr)$ill)
  
  # change in incomplete log-likelihood
  delta.ll = 1
  
  # number of iterations
  n.iter = 1
  
  # If the log-likelihood has changed by less than epsilon, EM will stop
  while ((delta.ll > epsilon) & (n.iter <= max.iter)) {
    
    # run the EM step 
    EM.out = EM.iter(X, X.more, w.curr, p.curr)
    
    # replace the current parameter estimates
    w.curr = EM.out$w.new
    p.curr = EM.out$p.new
    
    # compute the change in incomplete log-likelihood 
    log_liks = c(log_liks, compute.log.lik(X, X.more, w.curr, p.curr)$ill)
    delta.ll = log_liks[length(log_liks)] - log_liks[length(log_liks) - 1]
    
    # increase the number of iterations
    n.iter = n.iter + 1
  }
  return(list(w.curr=w.curr, p.curr=p.curr, log_liks=log_liks))
}

# EM-iteration
EM.iter = function(X, X.more, w.curr, p.curr) {

  # E-step
  prob.x.z = compute.prob.x.z(X, X.more, w.curr, p.curr)$prob.x.z
  P_ik = (prob.x.z / rowSums(prob.x.z))[1:300,]
  
  # M-step
  w.new = colSums(P_ik[1:300,]) / sum(P_ik[1:300,]) 
  p.new = colSums((P_ik * X)[1:300,]) / colSums(P_ik[1:300,]) / 20
  p1.new = (colSums((P_ik * X)[1:300,])[1] + sum(X.more)) / 
    (20 * (colSums(P_ik[seq(1,300),])[1] + 100))
  
  return(list(w.new=w.new, p.new=c(p1.new, p.new[2], p.new[3])))
}

# Compute Incomplete Log-likelihood 
compute.log.lik = function(X, X.more, w.curr, p.curr) {
  
  # compute probabilities
  prob.x.z = compute.prob.x.z(X, X.more, w.curr, p.curr)$prob.x.z
  
  # incomplete log-likelihoods
  ill = sum(log(rowSums(prob.x.z)))
  
  return(list(ill=ill))
}

# Compute probabilities
compute.prob.x.z = function(X, X.more, w.curr, p.curr) {

  L = matrix(0, nrow=(length(X) + length(X.more)), ncol=length(w.curr)) 
  for (i in 1:length(X)) {
    for (k in 1:ncol(L)) {
      L[i,k] = dbinom(X[i], size=20, prob=p.curr[k]) * w.curr[k]
    }
  }
  for (i in 1:length(X.more)) {
    L[i+length(X),1] = dbinom(X.more[i], size=20, prob=p.curr[1])
  }
  return(list(prob.x.z=L))
}
```

Apply the EM algorithm
```{r}
EM1 = mixture.EM(X, X.more, w.init=c(0.3,0.3,0.4), p.init=c(0.2, 0.5, 0.7))
EM2 = mixture.EM(X, X.more, w.init=c(0.1,0.2,0.7), p.init=c(0.1, 0.3, 0.7))
```

Print results 
```{r}
print.results <- function(EM) {
  print(paste("Estimate pi = (", round(EM$w.curr[1],2), ",", 
            round(EM$w.curr[2],2), ",",
            round(EM$w.curr[3],2), ")", sep=""))
  print(paste("Estimate p = (", round(EM$p.curr[1],2), ",", 
              round(EM$p.curr[2],2), ",",
              round(EM$p.curr[3],2), ")", sep=""))
  plot(EM$log_liks, ylab="incomplete log-likelihood", xlab="iteration")
}
print.results(EM1)
print.results(EM2)
```